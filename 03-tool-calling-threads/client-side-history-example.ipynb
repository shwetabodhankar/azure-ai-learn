{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929fcd43",
   "metadata": {},
   "source": [
    "# Chat Completion Style History Management Example\n",
    "\n",
    "This example demonstrates how conversation history is managed when using **Chat Completion style services** like AzureAIAgentClient with ChatAgent.\n",
    "\n",
    "**Key Difference (vs Azure AI Agent service):**\n",
    "- The **agent framework** manages conversation history transmission\n",
    "- Full conversation history is sent with each request to the underlying model\n",
    "- The service receives complete context every time (stateless model calls)\n",
    "- Thread storage may still happen on Azure, but the API pattern is different\n",
    "\n",
    "**Important Note:** \n",
    "Even though we use `AzureAIAgentClient`, threads may still be visible in Azure AI Foundry portal. The key difference is in the **API communication pattern**, not necessarily the storage location.\n",
    "\n",
    "**Required Environment Variables:**\n",
    "- `AZURE_AI_PROJECT_ENDPOINT`: Your Azure AI project endpoint\n",
    "- `AZURE_AI_MODEL_DEPLOYMENT_NAME`: The name of your model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7adae0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project endpoint: https://aifoundryaveva.services.ai.azure.com/api/projects/firstProject\n",
      "Deployment name: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from random import randint\n",
    "from agent_framework import ChatAgent\n",
    "from agent_framework.azure import AzureAIAgentClient\n",
    "from azure.identity.aio import AzureCliCredential\n",
    "\n",
    "project_endpoint = os.environ.get('AZURE_AI_PROJECT_ENDPOINT')\n",
    "model_name = os.environ.get('AZURE_AI_MODEL_DEPLOYMENT_NAME')\n",
    "\n",
    "print(f\"Project endpoint: {project_endpoint}\")\n",
    "print(f\"Deployment name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50f4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé≤ Tool Function: Weather Information\n",
    "def get_weather_info(city: str) -> str:\n",
    "    \"\"\"Get weather information for a city.\n",
    "    \n",
    "    Args:\n",
    "        city (str): The name of the city\n",
    "        \n",
    "    Returns:\n",
    "        str: Weather information for the specified city\n",
    "    \"\"\"\n",
    "    # Simulated weather data\n",
    "    weather_conditions = [\"sunny\", \"cloudy\", \"rainy\", \"partly cloudy\", \"snowy\"]\n",
    "    temp = randint(15, 35)  # Temperature in Celsius\n",
    "    condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n",
    "    \n",
    "    return f\"The weather in {city} is {condition} with a temperature of {temp}¬∞C.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fd617",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demonstrate_chat_completion_style():\n",
    "    \"\"\"\n",
    "    Demonstrates Chat Completion style conversation management.\n",
    "    The agent framework sends full conversation history with each request.\n",
    "    \"\"\"\n",
    "    print(\"=== CHAT COMPLETION STYLE DEMO ===\")\n",
    "    print(\"In this approach:\")\n",
    "    print(\"1. Agent framework manages conversation history transmission\")\n",
    "    print(\"2. Full conversation context is sent to the model with each request\")\n",
    "    print(\"3. Underlying model receives complete conversation every time\")\n",
    "    print(\"4. Thread may still be stored on Azure, but API pattern differs\\n\")\n",
    "    \n",
    "    credential = AzureCliCredential()\n",
    "    chat_client = AzureAIAgentClient(async_credential=credential)\n",
    "    \n",
    "    agent = ChatAgent(\n",
    "        chat_client, \n",
    "        name=\"Weather Assistant\",\n",
    "        instructions=\"You are a helpful weather assistant. Use the weather tool to provide accurate information.\",\n",
    "        tools=[get_weather_info]\n",
    "    )\n",
    "    \n",
    "    # Create a new thread\n",
    "    thread = agent.get_new_thread()\n",
    "    print(f\"üìù Created new thread object\")\n",
    "    print(f\"üåê Note: Thread may be stored on Azure, but communication style differs\")\n",
    "    print(f\"Thread contains: {len(thread.messages) if hasattr(thread, 'messages') else 0} messages initially\\n\")\n",
    "    \n",
    "    # First interaction\n",
    "    print(\"üîÑ FIRST REQUEST:\")\n",
    "    print(\"User: What's the weather like in Paris?\")\n",
    "    result1 = await agent.run(\"What's the weather like in Paris?\", thread=thread)\n",
    "    print(f\"Assistant: {result1.text}\")\n",
    "    print(f\"üìä Thread now contains: {len(thread.messages) if hasattr(thread, 'messages') else 'multiple'} messages\")\n",
    "    print(f\"üí° Framework sends: Just the current message to start conversation\\n\")\n",
    "    \n",
    "    # Second interaction - builds on previous context\n",
    "    print(\"üîÑ SECOND REQUEST (with context):\")\n",
    "    print(\"User: How about in London? Is it warmer there?\")\n",
    "    print(\"üì§ Framework behavior: Sends FULL conversation history to model\")\n",
    "    print(\"üì° API call includes: [Paris question + answer + London question]\")\n",
    "    result2 = await agent.run(\"How about in London? Is it warmer there?\", thread=thread)\n",
    "    print(f\"Assistant: {result2.text}\")\n",
    "    print(f\"üìä Thread now contains: {len(thread.messages) if hasattr(thread, 'messages') else 'even more'} messages\\n\")\n",
    "    \n",
    "    # Third interaction - continues building context\n",
    "    print(\"üîÑ THIRD REQUEST (with full context):\")\n",
    "    print(\"User: Which city would be better for a picnic?\")\n",
    "    print(\"üì§ Framework behavior: Sends COMPLETE conversation history\")\n",
    "    print(\"üì° API call includes: [All previous messages + new picnic question]\")\n",
    "    result3 = await agent.run(\"Which city would be better for a picnic?\", thread=thread)\n",
    "    print(f\"Assistant: {result3.text}\")\n",
    "    \n",
    "    # Let's check if we can see the thread ID (if it's stored on Azure)\n",
    "    thread_id = getattr(thread, 'id', 'Not available')\n",
    "    print(f\"\\nüìä THREAD INFORMATION:\")\n",
    "    print(f\"Thread ID: {thread_id}\")\n",
    "    print(f\"Thread may be visible in Azure AI Foundry portal\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üí° KEY INSIGHTS (Chat Completion Style):\")\n",
    "    print(\"‚Ä¢ Framework sends COMPLETE conversation history with each request\")\n",
    "    print(\"‚Ä¢ Model receives full context every time (stateless model calls)\")\n",
    "    print(\"‚Ä¢ Network payload grows with conversation length\")\n",
    "    print(\"‚Ä¢ Different from pure Azure AI Agent service API pattern\")\n",
    "    print(\"‚Ä¢ Thread storage location may vary by implementation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    await chat_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32739a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLIENT-SIDE HISTORY STORAGE DEMO ===\n",
      "In this approach:\n",
      "1. Conversation history is stored in the local 'thread' object\n",
      "2. Full history is sent to the service with each request\n",
      "3. Service is stateless - no memory of previous calls\n",
      "\n",
      "üìù Created new thread object (stored locally)\n",
      "Thread contains: 0 messages initially\n",
      "\n",
      "üîÑ FIRST REQUEST:\n",
      "User: What's the weather like in Paris?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-29 00:18:50 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n",
      "[2025-10-29 00:18:53 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n",
      "[2025-10-29 00:18:53 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The weather in Paris is currently sunny with a temperature of 17¬∞C.\n",
      "üìä Thread now contains: multiple messages\n",
      "\n",
      "üîÑ SECOND REQUEST (with context):\n",
      "User: How about in London? Is it warmer there?\n",
      "üì§ Sending to service: ENTIRE conversation history (Paris + London question)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-29 00:18:55 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n",
      "[2025-10-29 00:18:58 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n",
      "[2025-10-29 00:18:58 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The weather in London is rainy with a temperature of 32¬∞C, making it significantly warmer than Paris at the moment.\n",
      "üìä Thread now contains: even more messages\n",
      "\n",
      "üîÑ THIRD REQUEST (with full context):\n",
      "User: Which city would be better for a picnic?\n",
      "üì§ Sending to service: ENTIRE conversation history (Paris + London + picnic question)\n",
      "Assistant: Paris would be a better choice for a picnic as the weather there is sunny with a pleasant temperature of 17¬∞C. In contrast, London is experiencing rain, which may not be ideal for outdoor activities.\n",
      "\n",
      "============================================================\n",
      "üí° KEY INSIGHTS:\n",
      "‚Ä¢ Each request sends the COMPLETE conversation history\n",
      "‚Ä¢ Network payload grows with conversation length\n",
      "‚Ä¢ Service has no memory - relies on sent context\n",
      "‚Ä¢ Thread object manages all state locally\n",
      "============================================================\n",
      "Assistant: Paris would be a better choice for a picnic as the weather there is sunny with a pleasant temperature of 17¬∞C. In contrast, London is experiencing rain, which may not be ideal for outdoor activities.\n",
      "\n",
      "============================================================\n",
      "üí° KEY INSIGHTS:\n",
      "‚Ä¢ Each request sends the COMPLETE conversation history\n",
      "‚Ä¢ Network payload grows with conversation length\n",
      "‚Ä¢ Service has no memory - relies on sent context\n",
      "‚Ä¢ Thread object manages all state locally\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the demonstration\n",
    "await demonstrate_chat_completion_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b8e3d7",
   "metadata": {},
   "source": [
    "## How Chat Completion Style History Works\n",
    "\n",
    "### Data Flow Visualization:\n",
    "\n",
    "```\n",
    "Request 1:\n",
    "Client ‚Üí [Agent Framework] ‚Üí [Message: \"What's the weather in Paris?\"] ‚Üí Model Service\n",
    "                                                                          ‚Üì\n",
    "Client ‚Üê [Agent Framework] ‚Üê [Response: \"Weather in Paris is sunny...\"] ‚Üê Model Service\n",
    "\n",
    "Request 2:\n",
    "Client ‚Üí [Agent Framework] ‚Üí [Full History: \n",
    "                              - \"What's the weather in Paris?\"\n",
    "                              - \"Weather in Paris is sunny...\"\n",
    "                              - \"How about in London?\"] ‚Üí Model Service\n",
    "                                                           ‚Üì\n",
    "Client ‚Üê [Agent Framework] ‚Üê [Response: \"London is cloudy...\"] ‚Üê Model Service\n",
    "\n",
    "Request 3:\n",
    "Client ‚Üí [Agent Framework] ‚Üí [Complete History: \n",
    "                              - All previous messages\n",
    "                              - \"Which city is better for picnic?\"] ‚Üí Model Service\n",
    "```\n",
    "\n",
    "### The Real Difference:\n",
    "\n",
    "**Chat Completion Style (AzureAIAgentClient + ChatAgent):**\n",
    "- Agent framework handles conversation state\n",
    "- Full conversation history sent to model with each request\n",
    "- Model receives complete context every call (stateless)\n",
    "- May still store threads on Azure for persistence\n",
    "\n",
    "**Azure AI Agent Service (Direct AIProjectClient):**\n",
    "- Azure service handles conversation state\n",
    "- Only thread ID + new message sent with each request\n",
    "- Service retrieves conversation history internally\n",
    "- Threads definitively stored on Azure\n",
    "\n",
    "### Key Insights:\n",
    "- **Both approaches** may store threads on Azure AI Foundry\n",
    "- **The difference** is in the API communication pattern\n",
    "- **Chat Completion style**: Full history in each API call\n",
    "- **Agent Service style**: Minimal payload, server manages context\n",
    "- **Your observation is correct**: Threads appear in portal for both!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
