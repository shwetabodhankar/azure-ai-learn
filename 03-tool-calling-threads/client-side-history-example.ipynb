{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929fcd43",
   "metadata": {},
   "source": [
    "# Chat Completion Style History Management Example\n",
    "\n",
    "This example demonstrates how conversation history is managed when using **Chat Completion style services** like AzureAIAgentClient with ChatAgent.\n",
    "\n",
    "**Key Difference (vs Azure AI Agent service):**\n",
    "- The **agent framework** manages conversation history transmission\n",
    "- Full conversation history is sent with each request to the underlying model\n",
    "- The service receives complete context every time (stateless model calls)\n",
    "- Thread storage may still happen on Azure, but the API pattern is different\n",
    "\n",
    "**Important Note:** \n",
    "Even though we use `AzureAIAgentClient`, threads may still be visible in Azure AI Foundry portal. The key difference is in the **API communication pattern**, not necessarily the storage location.\n",
    "\n",
    "**Required Environment Variables:**\n",
    "- `AZURE_AI_PROJECT_ENDPOINT`: Your Azure AI project endpoint\n",
    "- `AZURE_AI_MODEL_DEPLOYMENT_NAME`: The name of your model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adae0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project endpoint: https://aifoundryaveva.services.ai.azure.com/api/projects/firstProject\n",
      "Deployment name: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from random import randint\n",
    "from agent_framework import ChatAgent\n",
    "from agent_framework.azure import AzureAIAgentClient\n",
    "from azure.identity.aio import AzureCliCredential\n",
    "\n",
    "project_endpoint = os.environ.get('AZURE_AI_PROJECT_ENDPOINT')\n",
    "model_name = os.environ.get('AZURE_AI_MODEL_DEPLOYMENT_NAME')\n",
    "\n",
    "print(f\"Project endpoint: {project_endpoint}\")\n",
    "print(f\"Deployment name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50f4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ² Tool Function: Weather Information\n",
    "def get_weather_info(city: str) -> str:\n",
    "    \"\"\"Get weather information for a city.\n",
    "    \n",
    "    Args:\n",
    "        city (str): The name of the city\n",
    "        \n",
    "    Returns:\n",
    "        str: Weather information for the specified city\n",
    "    \"\"\"\n",
    "    # Simulated weather data\n",
    "    weather_conditions = [\"sunny\", \"cloudy\", \"rainy\", \"partly cloudy\", \"snowy\"]\n",
    "    temp = randint(15, 35)  # Temperature in Celsius\n",
    "    condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n",
    "    \n",
    "    return f\"The weather in {city} is {condition} with a temperature of {temp}Â°C.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9fd617",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demonstrate_chat_completion_style():\n",
    "    \"\"\"\n",
    "    Demonstrates Chat Completion style conversation management.\n",
    "    The agent framework sends full conversation history with each request.\n",
    "    \"\"\"\n",
    "    print(\"=== CHAT COMPLETION STYLE DEMO ===\")\n",
    "    print(\"In this approach:\")\n",
    "    print(\"1. Agent framework manages conversation history transmission\")\n",
    "    print(\"2. Full conversation context is sent to the model with each request\")\n",
    "    print(\"3. Underlying model receives complete conversation every time\")\n",
    "    print(\"4. Thread may still be stored on Azure, but API pattern differs\\n\")\n",
    "    \n",
    "    credential = AzureCliCredential()\n",
    "    chat_client = AzureAIAgentClient(async_credential=credential)\n",
    "    \n",
    "    agent = ChatAgent(\n",
    "        chat_client, \n",
    "        name=\"Weather Assistant\",\n",
    "        instructions=\"You are a helpful weather assistant. Use the weather tool to provide accurate information.\",\n",
    "        tools=[get_weather_info]\n",
    "    )\n",
    "    \n",
    "    # Create a new thread\n",
    "    thread = agent.get_new_thread()\n",
    "    print(f\"ğŸ“ Created new thread object\")\n",
    "    print(f\"ğŸŒ Note: Thread may be stored on Azure, but communication style differs\")\n",
    "    print(f\"Thread contains: {len(thread.messages) if hasattr(thread, 'messages') else 0} messages initially\\n\")\n",
    "    \n",
    "    # First interaction\n",
    "    print(\"ğŸ”„ FIRST REQUEST:\")\n",
    "    print(\"User: What's the weather like in Paris?\")\n",
    "    result1 = await agent.run(\"What's the weather like in Paris?\", thread=thread)\n",
    "    print(f\"Assistant: {result1.text}\")\n",
    "    print(f\"ğŸ“Š Thread now contains: {len(thread.messages) if hasattr(thread, 'messages') else 'multiple'} messages\")\n",
    "    print(f\"ğŸ’¡ Framework sends: Just the current message to start conversation\\n\")\n",
    "    \n",
    "    # Second interaction - builds on previous context\n",
    "    print(\"ğŸ”„ SECOND REQUEST (with context):\")\n",
    "    print(\"User: How about in London? Is it warmer there?\")\n",
    "    print(\"ğŸ“¤ Framework behavior: Sends FULL conversation history to model\")\n",
    "    print(\"ğŸ“¡ API call includes: [Paris question + answer + London question]\")\n",
    "    result2 = await agent.run(\"How about in London? Is it warmer there?\", thread=thread)\n",
    "    print(f\"Assistant: {result2.text}\")\n",
    "    print(f\"ğŸ“Š Thread now contains: {len(thread.messages) if hasattr(thread, 'messages') else 'even more'} messages\\n\")\n",
    "    \n",
    "    # Third interaction - continues building context\n",
    "    print(\"ğŸ”„ THIRD REQUEST (with full context):\")\n",
    "    print(\"User: Which city would be better for a picnic?\")\n",
    "    print(\"ğŸ“¤ Framework behavior: Sends COMPLETE conversation history\")\n",
    "    print(\"ğŸ“¡ API call includes: [All previous messages + new picnic question]\")\n",
    "    result3 = await agent.run(\"Which city would be better for a picnic?\", thread=thread)\n",
    "    print(f\"Assistant: {result3.text}\")\n",
    "    \n",
    "    # Let's check if we can see the thread ID (if it's stored on Azure)\n",
    "    thread_id = getattr(thread, 'id', 'Not available')\n",
    "    print(f\"\\nğŸ“Š THREAD INFORMATION:\")\n",
    "    print(f\"Thread ID: {thread_id}\")\n",
    "    print(f\"Thread may be visible in Azure AI Foundry portal\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ’¡ KEY INSIGHTS (Chat Completion Style):\")\n",
    "    print(\"â€¢ Framework sends COMPLETE conversation history with each request\")\n",
    "    print(\"â€¢ Model receives full context every time (stateless model calls)\")\n",
    "    print(\"â€¢ Network payload grows with conversation length\")\n",
    "    print(\"â€¢ Different from pure Azure AI Agent service API pattern\")\n",
    "    print(\"â€¢ Thread storage location may vary by implementation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    #await chat_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32739a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHAT COMPLETION STYLE DEMO ===\n",
      "In this approach:\n",
      "1. Agent framework manages conversation history transmission\n",
      "2. Full conversation context is sent to the model with each request\n",
      "3. Underlying model receives complete conversation every time\n",
      "4. Thread may still be stored on Azure, but API pattern differs\n",
      "\n",
      "ğŸ“ Created new thread object\n",
      "ğŸŒ Note: Thread may be stored on Azure, but communication style differs\n",
      "Thread contains: 0 messages initially\n",
      "\n",
      "ğŸ”„ FIRST REQUEST:\n",
      "User: What's the weather like in Paris?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-29 01:50:41 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n",
      "[2025-10-29 01:50:43 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n",
      "[2025-10-29 01:50:43 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The weather in Paris is currently rainy with a temperature of 29Â°C.\n",
      "ğŸ“Š Thread now contains: multiple messages\n",
      "ğŸ’¡ Framework sends: Just the current message to start conversation\n",
      "\n",
      "ğŸ”„ SECOND REQUEST (with context):\n",
      "User: How about in London? Is it warmer there?\n",
      "ğŸ“¤ Framework behavior: Sends FULL conversation history to model\n",
      "ğŸ“¡ API call includes: [Paris question + answer + London question]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-29 01:50:45 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n",
      "[2025-10-29 01:50:47 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n",
      "[2025-10-29 01:50:47 - c:\\Aveva\\Samples\\azure-ai-learn\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:609 - WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The weather in London is currently partly cloudy with a temperature of 23Â°C, which is cooler than Paris where it is 29Â°C.\n",
      "ğŸ“Š Thread now contains: even more messages\n",
      "\n",
      "ğŸ”„ THIRD REQUEST (with full context):\n",
      "User: Which city would be better for a picnic?\n",
      "ğŸ“¤ Framework behavior: Sends COMPLETE conversation history\n",
      "ğŸ“¡ API call includes: [All previous messages + new picnic question]\n",
      "Assistant: For a picnic, London would be a better choice since itâ€™s partly cloudy with cooler weather at 23Â°C, providing a pleasant environment. Paris, on the other hand, is rainy, which isn't ideal for outdoor activities.\n",
      "\n",
      "ğŸ“Š THREAD INFORMATION:\n",
      "Thread ID: Not available\n",
      "Thread may be visible in Azure AI Foundry portal\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¡ KEY INSIGHTS (Chat Completion Style):\n",
      "â€¢ Framework sends COMPLETE conversation history with each request\n",
      "â€¢ Model receives full context every time (stateless model calls)\n",
      "â€¢ Network payload grows with conversation length\n",
      "â€¢ Different from pure Azure AI Agent service API pattern\n",
      "â€¢ Thread storage location may vary by implementation\n",
      "======================================================================\n",
      "Assistant: For a picnic, London would be a better choice since itâ€™s partly cloudy with cooler weather at 23Â°C, providing a pleasant environment. Paris, on the other hand, is rainy, which isn't ideal for outdoor activities.\n",
      "\n",
      "ğŸ“Š THREAD INFORMATION:\n",
      "Thread ID: Not available\n",
      "Thread may be visible in Azure AI Foundry portal\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¡ KEY INSIGHTS (Chat Completion Style):\n",
      "â€¢ Framework sends COMPLETE conversation history with each request\n",
      "â€¢ Model receives full context every time (stateless model calls)\n",
      "â€¢ Network payload grows with conversation length\n",
      "â€¢ Different from pure Azure AI Agent service API pattern\n",
      "â€¢ Thread storage location may vary by implementation\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the demonstration\n",
    "await demonstrate_chat_completion_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b8e3d7",
   "metadata": {},
   "source": [
    "## How Chat Completion Style History Works\n",
    "\n",
    "### Data Flow Visualization:\n",
    "\n",
    "```\n",
    "Request 1:\n",
    "Client â†’ [Agent Framework] â†’ [Message: \"What's the weather in Paris?\"] â†’ Model Service\n",
    "                                                                          â†“\n",
    "Client â† [Agent Framework] â† [Response: \"Weather in Paris is sunny...\"] â† Model Service\n",
    "\n",
    "Request 2:\n",
    "Client â†’ [Agent Framework] â†’ [Full History: \n",
    "                              - \"What's the weather in Paris?\"\n",
    "                              - \"Weather in Paris is sunny...\"\n",
    "                              - \"How about in London?\"] â†’ Model Service\n",
    "                                                           â†“\n",
    "Client â† [Agent Framework] â† [Response: \"London is cloudy...\"] â† Model Service\n",
    "\n",
    "Request 3:\n",
    "Client â†’ [Agent Framework] â†’ [Complete History: \n",
    "                              - All previous messages\n",
    "                              - \"Which city is better for picnic?\"] â†’ Model Service\n",
    "```\n",
    "\n",
    "### The Real Difference:\n",
    "\n",
    "**Chat Completion Style (AzureAIAgentClient + ChatAgent):**\n",
    "- Agent framework handles conversation state\n",
    "- Full conversation history sent to model with each request\n",
    "- Model receives complete context every call (stateless)\n",
    "- May still store threads on Azure for persistence\n",
    "\n",
    "**Azure AI Agent Service (Direct AIProjectClient):**\n",
    "- Azure service handles conversation state\n",
    "- Only thread ID + new message sent with each request\n",
    "- Service retrieves conversation history internally\n",
    "- Threads definitively stored on Azure\n",
    "\n",
    "### Key Insights:\n",
    "- **Both approaches** may store threads on Azure AI Foundry\n",
    "- **The difference** is in the API communication pattern\n",
    "- **Chat Completion style**: Full history in each API call\n",
    "- **Agent Service style**: Minimal payload, server manages context\n",
    "- **Your observation is correct**: Threads appear in portal for both!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
